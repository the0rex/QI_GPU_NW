pipeline:
  name: "GPU-ACCELERATED nw_chunked_pipeline with energy-based Quantum-Inspired Scoring"
  version: "1.0.0"
  description: "GPU-accelerated alignment with fallback safety"

# Input/Output settings
io:
  fasta_dir: "fasta_sequences/"
  default_fasta1: "NZ_JAIVNO010000161.1.fa"
  default_fasta2: "NZ_JAIVNO010000161.3.fa"
  output_dir: "Results_GPU/"
  chunk_dir: "chunks_tmp_gpu/"
  visualization_dir: "Results_GPU/visualizations/"
  logs_dir: "Results_GPU/logs/"
  gpu_results_dir: "Results_GPU/performance/"
  temp_dir: "Data/temp_chromosomes/"  # Add this line
  chromosome_separator: ""  # Optional: add separator between combined chromosomes (e.g., "N"*100)

# Alignment parameters (GPU-optimized)
alignment:
  gap_open: -30
  gap_extend: -0.5
  match_score: 5
  mismatch_score: -40
  window_size_predictor: "gpu"  # Options: gpu, cpu, tf, simple
  beam_width: 30
  carry_gap_state: true
  use_gpu_acceleration: true
  gpu_memory_fraction: 0.9
  enable_mixed_precision: true
  
# NW Affine algorithm parameters
nw_affine:
  tau: 3.0
  energy_match: -5.0
  energy_mismatch: 40.0
  energy_gap_open: 30.0
  energy_gap_extend: 0.5
  carry_gap_penalty: true
  max_depth: 3  # for re-anchoring
  
# Beam search parameters
beam_search:
  beam_width: 100
  window_size_multiplier: 0.1  # 10% of sequence length
  min_window_size: 10
  max_window_size: 500
  
# Re-anchoring parameters
reanchoring:
  max_internal_anchors: 50
  min_anchor_distance: 10
  max_gap_run: 30
  max_gap_fraction: 0.15

# Chunking parameters (GPU-optimized)
chunking:
  default_chunk_size: 5000  # Increased for better GPU utilization
  overlap: 500
  min_chunk_size: 500
  max_chunk_size: 50000  # Increased for GPU
  chunk_tolerance: 0.05
  use_gpu_anchoring: true
  use_gpu_pruning: true
  gpu_chunk_batch_size: 32
  sequential_processing: true

# Seeding parameters (GPU-optimized)
seeding:
  kmer_size: 21
  syncmer_size: 5
  syncmer_position: 2
  window_min: 20
  window_max: 70
  max_occurrences: 200
  use_gpu_hashing: true
  gpu_hash_table_size: 1024

# Model paths (for window prediction)
models:
  window_predictor: "model_Adam.h5"
  torch_window_predictor: "model_torch.pt"
  x_scaler: "x_scaler.pkl"
  y_scaler: "y_scaler.pkl"
  gpu_model_enabled: true

# GPU-specific settings
gpu:
  enabled: true
  device_id: 0
  cuda_benchmark: true
  memory_growth: true
  allow_memory_growth: true
  per_process_gpu_memory_fraction: 0.9
  gpu_debug_verbosity: 0
  cuda_launch_blocking: false
  
  # PyTorch GPU settings
  torch:
    cudnn_benchmark: true
    cudnn_deterministic: false
    allow_tf32: true
  
  # TensorFlow/Keras GPU settings  
  tensorflow:
    log_device_placement: false
    allow_growth: true
    memory_limit: null  # Auto
    xla_auto_jit: 2

# Performance settings (GPU-optimized)
performance:
  use_multiprocessing: true
  num_workers: "auto"  # Will be adjusted based on GPU availability
  chunk_buffer_size: 20  # Increased for GPU
  cache_enabled: true
  cache_maxsize: 4  # Increased for GPU batches
  use_gpu: true
  gpu_batch_size: 32
  gpu_streams: 4
  enable_async_io: true
  prefetch_factor: 2
  
  # Memory optimization
  pin_memory: true
  num_io_threads: 4
  disable_gc_during_alignment: true

# Validation settings
validation:
  validate_inputs: true
  check_memory: true
  max_sequence_length: 300000000  # Increased for GPU processing
  min_sequence_length: 100
  check_gpu_memory: true
  min_gpu_memory_gb: 4.0
  validate_gpu_capability: true
  cuda_compute_capability: 7.0
  
  # Disk requirements for GPU processing
  required_disk_gb: 20.0  # Increased for GPU temp files
  min_cpu_cores: 4  # Increased for GPU data feeding
  min_memory_gb: 8.0  # Increased for GPU buffers

# Debug settings
debug:
  verbose: false  # Reduced for cleaner output
  save_intermediate: false
  log_level: "INFO"
  profile_performance: true
  profile_gpu: true
  log_cuda_memory: true
  trace_gpu_operations: false
  
  # Suppress warnings
  suppress_tensorflow_warnings: true
  suppress_pytorch_warnings: true
  suppress_numpy_warnings: true
  
  # Performance profiling
  enable_profiler: false
  profiler_output: "Results_GPU/profiler/"
  profile_memory: true

# Visualization settings
visualization:
  enabled: true
  create_all: true
  interactive_html: true
  save_raw_data: true
  gpu_performance_charts: true
  memory_usage_plots: true
  
  # GPU-specific visualizations
  plot_gpu_utilization: true
  plot_memory_timeline: true
  compare_gpu_cpu: true

# Fallback settings (if GPU fails)
fallback:
  enable_cpu_fallback: true
  cpu_warning_threshold: 0.8  # Switch to CPU if GPU memory > 80%
  gpu_error_retry_count: 3
  fallback_to_cpu_on_error: true
  warn_on_fallback: true
  
  # CPU fallback parameters
  cpu:
    num_workers: 4
    chunk_size: 5000
    beam_width: 20

# Optimization profiles
optimization:
  mode: "balanced"  # Options: performance, balanced, memory
  precision: "mixed"  # Options: mixed, fp32, fp16
  
  # Performance tuning
  enable_auto_tuning: true
  auto_batch_size: true
  dynamic_chunk_sizing: true
  
  # Memory optimization
  gradient_checkpointing: false
  activation_checkpointing: false
  memory_efficient_attention: true

# Monitoring
monitoring:
  enable_realtime_monitoring: true
  log_interval_seconds: 10
  monitor_gpu_temperature: false
  alert_on_overheating: false
  temperature_threshold: 85
  
  # Resource monitoring
  track_cpu_usage: true
  track_gpu_usage: true
  track_memory_usage: true
  save_monitoring_data: true